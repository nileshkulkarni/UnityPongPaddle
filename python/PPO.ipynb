{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 35e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo_wind\" # The sub-directory name for model and summary statistics\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"pong_wind\" # Name of the training environment file.\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 1e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 64 # How many experiences per gradient descent update step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents.environment:\n",
      "'PongAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: PongAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: PongBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 8\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: x_rotate, z_rotate\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "print(str(env))\n",
    "brain_name = env.brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo_wind\\model-2450000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo_wind\\model-2450000.cptk\n",
      "c:\\users\\niles\\anaconda2\\envs\\ml-agents\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\users\\niles\\anaconda2\\envs\\ml-agents\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: nan\n",
      "Saved Model\n",
      "Mean Reward: 10.879879709385925\n",
      "Mean Reward: 9.46524049241424\n",
      "Mean Reward: 15.950701609192855\n",
      "Mean Reward: 14.146819595787058\n",
      "Mean Reward: 13.467401845317474\n",
      "Saved Model\n",
      "Mean Reward: 11.508646859307209\n",
      "Mean Reward: 16.266712965354742\n",
      "Mean Reward: 12.737212650354367\n",
      "Mean Reward: 12.403202352372935\n",
      "Mean Reward: 21.09162005081437\n",
      "Saved Model\n",
      "Mean Reward: 8.746325073187212\n",
      "Mean Reward: 11.772374663908053\n",
      "Mean Reward: 18.31317798857413\n",
      "Mean Reward: 20.469890155435362\n",
      "Mean Reward: 10.668777768771506\n",
      "Saved Model\n",
      "Mean Reward: 13.100830623396977\n",
      "Mean Reward: 10.024443534829587\n",
      "Mean Reward: 23.446146117406943\n",
      "Mean Reward: 21.980997945658313\n",
      "Mean Reward: 26.93546790955476\n",
      "Saved Model\n",
      "Mean Reward: 25.11787896583384\n",
      "Mean Reward: 18.781897533690024\n",
      "Mean Reward: 4.822904101619503\n",
      "Mean Reward: 18.370016897329474\n",
      "Mean Reward: 16.636392854379647\n",
      "Saved Model\n",
      "Mean Reward: 12.867708515522649\n",
      "Mean Reward: 27.56214940577874\n",
      "Mean Reward: 19.269856917360396\n",
      "Mean Reward: 21.199917105961838\n",
      "Mean Reward: 27.10960116421383\n",
      "Saved Model\n",
      "Mean Reward: 29.84796634531521\n",
      "Mean Reward: 24.947768750986643\n",
      "Mean Reward: 27.45297573826272\n",
      "Mean Reward: 23.221494203597647\n",
      "Mean Reward: 23.77122805199811\n",
      "Saved Model\n",
      "Mean Reward: 11.423421794874793\n",
      "Mean Reward: 20.096578646767686\n",
      "Mean Reward: 21.40754846229283\n",
      "Mean Reward: 16.959981706932897\n",
      "Mean Reward: 30.368263834255327\n",
      "Saved Model\n",
      "Mean Reward: 32.242607006661494\n",
      "Mean Reward: 31.847289888765268\n",
      "Mean Reward: 31.485853670761088\n",
      "Mean Reward: 20.17707603349846\n",
      "Mean Reward: 21.393822263482246\n",
      "Saved Model\n",
      "Mean Reward: 27.035040900128227\n",
      "Mean Reward: 30.82563038084289\n",
      "Mean Reward: 26.619911409201162\n",
      "Mean Reward: 27.91388514032516\n",
      "Mean Reward: 25.672497147935935\n",
      "Saved Model\n",
      "Mean Reward: 17.408984931536462\n",
      "Mean Reward: 24.285691252570935\n",
      "Mean Reward: 26.160807053076827\n",
      "Mean Reward: 23.33459863204197\n",
      "Mean Reward: 14.861226161997768\n",
      "Saved Model\n",
      "Mean Reward: 19.565286760859557\n",
      "Mean Reward: 18.187091209794207\n",
      "Mean Reward: 11.03495853535043\n",
      "Mean Reward: 14.591020176342935\n",
      "Mean Reward: 24.16960380349281\n",
      "Saved Model\n",
      "Mean Reward: 21.736361575450882\n",
      "Mean Reward: 25.783825213681823\n",
      "Mean Reward: 22.82060228854812\n",
      "Mean Reward: 21.9650559297659\n",
      "Mean Reward: 29.458586537838304\n",
      "Saved Model\n",
      "Mean Reward: 24.996508101881567\n",
      "Mean Reward: 27.42494326618086\n",
      "Mean Reward: 15.969377173847493\n",
      "Mean Reward: 15.511900131773814\n",
      "Mean Reward: 26.081083898667206\n",
      "Saved Model\n",
      "Mean Reward: 30.189468531857568\n",
      "Mean Reward: 34.7562156253514\n",
      "Mean Reward: 34.0406170524079\n",
      "Mean Reward: 32.65268725688043\n",
      "Mean Reward: 35.4227499882575\n",
      "Saved Model\n",
      "Mean Reward: 32.909894886107885\n",
      "Mean Reward: 32.735238503166926\n",
      "Mean Reward: 32.18234839682536\n",
      "Mean Reward: 33.4269601397915\n",
      "Mean Reward: 32.679537524899615\n",
      "Saved Model\n",
      "Mean Reward: 35.19706921803719\n",
      "Mean Reward: 35.08790179144285\n",
      "Mean Reward: 32.517182087864256\n",
      "Mean Reward: 27.452233571824873\n",
      "Mean Reward: 27.20797159812699\n",
      "Saved Model\n",
      "Mean Reward: 26.33767582430686\n",
      "Mean Reward: 16.444139276486883\n",
      "Mean Reward: 21.127364092122747\n",
      "Mean Reward: 25.10944141266563\n",
      "Mean Reward: 25.763450630781506\n",
      "Saved Model\n",
      "Mean Reward: 28.043393230979316\n",
      "Mean Reward: 25.82896051135984\n",
      "Mean Reward: 30.20894001794692\n",
      "Mean Reward: 31.543666537280277\n",
      "Mean Reward: 32.11918688696229\n",
      "Saved Model\n",
      "Mean Reward: 30.76919236836166\n",
      "Mean Reward: 32.16070586183944\n",
      "Mean Reward: 30.201190306961454\n",
      "Mean Reward: 30.977788626301304\n",
      "Mean Reward: 31.40110169885872\n",
      "Saved Model\n",
      "Mean Reward: 31.635801871608734\n",
      "Mean Reward: 28.377928106595007\n",
      "Mean Reward: 26.965194328605875\n",
      "Mean Reward: 28.059949996988916\n",
      "Mean Reward: 29.499983924279576\n",
      "Saved Model\n",
      "Saved Model\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo_wind\\model-3500001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo_wind\\model-3500001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps = sess.run(ppo_model.global_step)\n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model)[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model)[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-400000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-400000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
